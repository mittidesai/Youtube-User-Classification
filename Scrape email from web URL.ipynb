{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/miti/.local/lib/python2.7/site-packages/bs4/__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"html.parser\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 174 of the file /usr/lib/python2.7/runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"html.parser\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scraping http://dkohreidze.com/jquery\n",
      "checking contact page for dkohreidze.com/jquery\n",
      "dkohreidze.com/jquery/http://dkohreidze.com/#contact\n",
      "\n",
      "Scraping dkohreidze.com/jquery/http://dkohreidze.com/#contact\n",
      "Cannot find email.\n",
      "\n",
      "Scraping http://rivercitiescustomconcepts.com\n",
      "williamw@rivercitiescustomconcepts.com,http://rivercitiescustomconcepts.com\n",
      "\n",
      "Scraping http://www.paintingsa.com/\n",
      "powellpainting@gmail.com,www.paintingsa.com/\n",
      "\n",
      "Scraping http://paradisegardentx.com/\n",
      "checking contact page for http://paradisegardentx.com/\n",
      "\n",
      "Scraping http://www.propaintingandstaining.com\n",
      "jimmy@propaintingandstaining.com,http://www.propaintingandstaining.com\n",
      "\n",
      "Scraping http://www.handhvinylfencing.com\n",
      "eric@handhvinylfencing.com,http://www.handhvinylfencing.com\n",
      "\n",
      "Scraping http://www.catrinasranchinteriors.com/\n",
      "checking contact page for http://www.catrinasranchinteriors.com/\n",
      "http://www.catrinasranchinteriors.com//contactus.html\n",
      "\n",
      "Scraping http://www.catrinasranchinteriors.com//contactus.html\n",
      "catrina@catrinasranchinteriors.com,http://www.catrinasranchinteriors.com/\n",
      "\n",
      "Sites scraped = 7\n",
      "Emails captured = 5\n",
      "Success rate = 71%\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import re\n",
    "import requests\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# scrapes the title \n",
    "def get_title():\n",
    "\td = soup.find_all(\"h1\", \"branded-page-header-title\")\n",
    "\tfor i in d:\n",
    "\t\ttitle = i.text.strip().replace('\\n',' ').replace(',','').encode(\"utf-8\") \n",
    "\t\tf.write(title+',')\n",
    "\t\tprint('\\t%s') % (title)\n",
    "\n",
    "# scrapes the subscriber count\n",
    "def get_subs():\n",
    "\tb = soup.find_all(\"span\", \"about-stat\")\n",
    "\tfor i in b:\n",
    "\t\ttry:\t\t\t\n",
    "\t\t\tvalue = i.b.text.strip().replace(',','')\t\t\t\t\t\n",
    "\t\t\tif len(b) == 3:\n",
    "\t\t\t\tf.write(value+',')\n",
    "\t\t\t\tprint('\\t%s') %(value)\n",
    "\t\t\telif len(b) == 2:\n",
    "\t\t\t\tf.write('null,'+ value + ',')\n",
    "\t\t\t\tprint('\\tsubs = null\\n\\t%s') %(value)\n",
    "\t\t\telse:\n",
    "\t\t\t\tf.write('null,null,')\n",
    "\t\t\t\tprint('\\tsubs = null\\nviews = null')\n",
    "\t\texcept AttributeError:\n",
    "\t\t\tpass\n",
    "\n",
    "# scrapes the description\n",
    "def get_description():\n",
    "\tc = soup.find_all(\"div\", \"about-description\")\n",
    "\tif c:\n",
    "\t\tfor i in c:\n",
    "\t\t\tdescription = i.text.strip().replace('\\n',' ').replace(',','').encode(\"utf-8\")\t\t\n",
    "\t\t\tf.write(description+',')\n",
    "\t\t\tprint('\\t%s') % (description)\n",
    "\t\t\t\n",
    "\n",
    "\t\t\tregex = re.compile((\"([a-z0-9!#$%&'*+\\/=?^_`{|}~-]+(?:\\.[a-z0-9!#$%&'*+\\/=?^_`\"\n",
    "\t\t\t\t\t\t        \"{|}~-]+)*(@|\\sat\\s|\\[at\\])(?:[a-z0-9](?:[a-z0-9-]*[a-z0-9])?(\\.|\"\n",
    "\t\t\t\t\t\t        \"\\sdot\\s))+[a-z0-9](?:[a-z0-9-]*[a-z0-9])?)\"))\n",
    "\t\t\t\n",
    "\t\t\temail = re.search(regex, description)\n",
    "\t\t\tif email:\n",
    "\t\t\t\tif not email.group(0).startswith('//'):\n",
    "\t\t\t\t\tprint('\\tEmail = ' + email.group())\n",
    "\t\t\t\t\tf.write(email.group(0)+',')\n",
    "\t\t\telse:\n",
    "\t\t\t\tprint('\\tEmail = null')\n",
    "\t\t\t\tf.write('null,')\n",
    "\telse:\n",
    "\t\tprint('\\tDescription = null\\n\\tEmail = null')\n",
    "\t\tf.write('null,null,')\n",
    "\n",
    "\t\t\n",
    "\n",
    "# scrapes all the external links \n",
    "def get_links():\n",
    "\ta = soup.find_all(\"a\", \"about-channel-link \") # trailing space is required.\n",
    "\tfor i in a:\n",
    "\t\turl = i.get('href')\n",
    "\t\tf.write(url+',')\n",
    "\t\tprint('\\t%s') % (url)\n",
    "\n",
    "# scrapes the related channels\n",
    "def get_related():\n",
    "\ts = soup.find_all(\"h3\", \"yt-lockup-title\")\n",
    "\tfor i in s:\n",
    "\t\tt = i.find_all(href=re.compile(\"user\"))\n",
    "\t\tfor i in t:\n",
    "\t\t\turl = 'https://www.youtube.com'+i.get('href')\n",
    "\t\t\trelated.write(url+'\\n')\n",
    "\t\t\t#print('\\t\\t%s,%s') % (i.text, url)\t\n",
    "\n",
    "\n",
    "def add_protocol(url):\n",
    "\tif (url.startswith(\"http://\") or url.startswith(\"https://\")): \n",
    "\t\treturn url\n",
    "\telse:\n",
    "\t\turl = 'http://' + url\n",
    "\t\treturn url\n",
    "\n",
    "f = open(\"email-scrape-data.csv\", \"w+\") \n",
    "s = ['dkohreidze.com/jquery', 'http://rivercitiescustomconcepts.com', 'www.paintingsa.com/', 'http://paradisegardentx.com/', 'http://www.propaintingandstaining.com', 'http://www.handhvinylfencing.com', 'http://www.catrinasranchinteriors.com/'] # enumerate all keywords here\n",
    "siteCount = 0\n",
    "emailCount = 0\n",
    "\n",
    "for site in s:\n",
    "\tsiteCount += 1\n",
    "\n",
    "\t# ping and retrieve home page HTML \n",
    "\tr = requests.get(add_protocol(site))\n",
    "\n",
    "\t# create Soup object from HTML \n",
    "\tsoup = BeautifulSoup(r.text)\n",
    "\n",
    "\tprint('\\nScraping %s') %(add_protocol(site))\n",
    "\n",
    "\tmailto = soup.find(href=re.compile(\"mailto\"))\n",
    "\n",
    "\tif mailto:\n",
    "\t\temail = re.sub('mailto:', '', mailto.get('href').lower()) \n",
    "\t\tf.write(email + ',' + site + '\\n')\n",
    "\t\tprint email + ',' + site\n",
    "\t\temailCount += 1\n",
    "\telse: # move on to contact page\n",
    "\t\tprint('checking contact page for %s') %site\n",
    "\t\t \n",
    "\t\tcontact = soup.find(href=re.compile(\"contact\"))\n",
    "\t\t\n",
    "\t\tif contact:\n",
    "\t\t\tif str(contact).startswith('/'):\n",
    "\t\t\t\tcontactURL = str(site + contact.get('href').lower())\n",
    "\t\t\t\tprint contactURL\n",
    "\t\t\telif str(contact).startswith('http'):\t\n",
    "\t\t\t\tcontactURL = contact.get('href').lower()\n",
    "\t\t\t\tprint contactURL\n",
    "\t\t\telse:\n",
    "\t\t\t\tcontactURL = str(site + '/' + contact.get('href').lower())\n",
    "\t\t\t\tprint contactURL\n",
    "\n",
    "\t\t\tr = requests.get(add_protocol(contactURL))\n",
    "\t\t\tsoup = BeautifulSoup(r.text)\n",
    "\t\t\t\n",
    "\t\t\tprint('\\nScraping %s') %(contactURL)\n",
    "\t\t\n",
    "\t\t\tmailto = soup.find(href=re.compile(\"mailto\"))\n",
    "\n",
    "\t\t\tif mailto:\n",
    "\t\t\t\temail = re.sub('mailto:', '', mailto.get('href').lower()) \n",
    "\t\t\t\tf.write(email + ',' + site + '\\n')\n",
    "\t\t\t\tprint email + ',' + site\n",
    "\t\t\t\temailCount += 1\n",
    "\t\t\telse:\n",
    "\t\t\t\tprint('Cannot find email.')\n",
    "\t\telse:\n",
    "\t\t\tpass\n",
    "\n",
    "successRate = float(emailCount) / float(siteCount) * 100.0\n",
    "print('\\nSites scraped = %d\\nEmails captured = %d\\nSuccess rate = %d%%') %(siteCount, emailCount, successRate)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
